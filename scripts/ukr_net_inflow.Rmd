---
title: "Upper Klamath Lake Net Inflow"
author: "Skyler Lewis"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
    toc_depth: 3
    number_sections: false
---

```{r setup}
library(tidyverse)
library(dplyr)

# install.packages("dataRetrieval")
library(dataRetrieval)
```

## USGS Data Import

```{r}
get_usgs_data <- function(site_number, parameter, statistic, start_date, end_date, workspace = here::here("data-raw", "usgs")) {
  
  parm_code <- sprintf("%05d", parameter)
  stat_code <- sprintf("%05d", statistic)
  
  filename <- str_glue("usgs_{site_number}_{parm_code}_{stat_code}_{format(start_date, '%Y%m%d')}_{format(end_date, '%Y%m%d')}.Rds")
  filepath <- file.path(workspace, filename)
  message(filepath)
  
  if(!dir.exists(workspace)) {
    dir.create(workspace, recursive = T)
  }
  
  if(!file.exists(filepath)) {
    data_raw <- dataRetrieval::readNWISdata(siteNumbers = site_number,
                                            parameterCd = parm_code,
                                            startDate = start_date,
                                            endDate = end_date)
    data_raw |> saveRDS(filepath)
  } else {
    data_raw <- readRDS(filepath)
  }
  
  data_raw |>
    rename(value = !!sym(str_glue("X_{parm_code}_{stat_code}")),
           cd = !!sym(str_glue("X_{parm_code}_{stat_code}_cd"))) |>
    dplyr::select(date_time = dateTime, value, cd)
}
```

USGS data-value qualification codes:

* A = Approved for publication -- Processing and review completed.
* P = Provisional data subject to revision.

```{r}
start_date <- ymd("1980-06-29")
end_date <- ymd("2025-3-24")

# USGS  11507001  UPPER KLAMATH LAKE NR K.FALLS(WEIGHT/MEAN ELEV) OR
# Lake or reservoir water surface elevation above NAVD 1988, feet
ukl_navd88 <- get_usgs_data(11507001, 62615, 00003, start_date, end_date)
# Lake or reservoir elevation above United States Bureau of Reclamation Klamath Basin (USBRKB) Datum, feet
ukl_usbrkb <- get_usgs_data(11507001, 72275, 00003, start_date, end_date)

# USGS	11507500	LINK RIVER AT KLAMATH FALLS, OR
# Discharge, cubic feet per second
link_q <- get_usgs_data(11507500, 00060, 00003, start_date, end_date)

# USGS  11507501  LINK RIVER BELOW KENO CANAL, NEAR KLAMATH FALLS, OR
# Discharge, cubic feet per second
link_keno_q <- get_usgs_data(11507501, 00060, 00003, start_date, end_date)
```

## Upper Klamath Lake Storage

### UKL Elevation-Capacity Rating Curves

#### UKL Bathymetry

UKL bathymetry measured in 2023 Tech Memo: w/ Tulana and Goose Bay, w/o Caledonia or ALB		
		
```{r}
ukl_bathy <- read_csv(here::here("data-raw", "elev_cap", "ukl_bathy.csv")) |>
  glimpse()
```

#### Raw Elevation Capacity Curves

Elevation capacity data for areas that have been, or may in the future be, temporarily or permanently re-connected to UKL.								

cal = Caledonia (See file "Caledonia 14Oct2016.xlsx")
tul = Tulana (See file "Tulana Nov2016.xlsx")
goo = Goose Bay (See file "GooseBay Nov2016.xlsx")
alb = Agency Lake Barnes (ALB) (See file "Agency Lake Barnes ET for KBPM v3 Aug2023 revised UKL net inflow.xlsx")

```{r}
elev_cap <- 
  read_csv(here::here("data-raw", "elev_cap", "elev_cap.csv")) |>
  glimpse()

# af = active storage volume (af)
# ac = inundated area (ac)

area_codes <- c("Caledonia" = "cal",
                "Tulana" = "tul",
                "Goose Bay" = "goo",
                "Agency Lake Barnes" = "alb")

# elev_cap |>
#   pivot_longer(cols = c(starts_with("af_"), starts_with("ac_"))) |>
#   separate_wider_delim(name, "_", names = c("name", "area_code")) |>
#   mutate(name = factor(name, 
#                        levels = c("af", "ac"), 
#                        labels = c("act_stor_vol_af", "inund_area_ac")),
#          site = factor(area_code, 
#                        levels = area_codes,
#                        labels = names(area_codes))) |>
#   pivot_wider()
```

#### Elevation Capacity for Storage Scenarios

* `s0`: Baseline: UKL w/o Caledonia, Tulana, Goose Bay or ALB
  * Caledonia reconnected 7/8/2006-12/31/2006
* `s1`: UKL w/ Caledonia, w/o Tulana, Goose Bay or ALB
  * Tulana reconnected 10/31/2007-forward
* `s2`: UKL w/ Tulana, w/o Caledonia, Goose Bay or ALB
  * Goose Bay reconnected 11/20/2008 -forward
* `s3`: UKL w/ Tulana and Goose Bay, w/o Caledonia or ALB
  * ALB proposed for reconnection
* `s4`: UKL w/ Tulana, Goose Bay and ALB, w/o Caledonia

```{r}
storage_scenarios <-
  ukl_bathy |>
  inner_join(elev_cap, by = join_by(ukl_ft_usbrkb)) |>
  transmute(ukl_ft_usbrkb,
            cap_taf_s0 = act_cap_taf - (af_tul / 1000) - (af_goo / 1000),                   # UKL w/o Caledonia, Tulana, Goose Bay or ALB
            cap_taf_s1 = act_cap_taf - (af_tul / 1000) - (af_goo / 1000) + (af_cal / 1000), # UKL w/ Caledonia, w/o Tulana, Goose Bay or ALB
            cap_taf_s2 = act_cap_taf - (af_goo / 1000),                                     # UKL w/ Tulana, w/o Caledonia, Goose Bay or ALB
            cap_taf_s3 = act_cap_taf,                                                       # UKL w/ Tulana and Goose Bay, w/o Caledonia or ALB
            cap_taf_s4 = act_cap_taf + (af_alb / 1000)) |>                                  # UKL w/ Tulana, Goose Bay and ALB, w/o Caledonia
  glimpse()
```
### UKL Active Storage Time Series

Apply the appropriate storage rating curve based on the time window.

```{r}
ukl_storage_ts <- ukl_usbrkb |> 
  dplyr::select(date_time, ukl_ft_usbrkb = value) |>
  inner_join(storage_scenarios, by = join_by(ukl_ft_usbrkb)) |>
  transmute(date_time,
            ukl_ft_usbrkb,
            ukl_act_cap_taf = case_when(date_time < ymd("2006-07-08") ~ cap_taf_s0, # UKL w/o Caledonia, Tulana, Goose Bay or ALB
                                        date_time < ymd("2007-01-01") ~ cap_taf_s1, # UKL w/ Caledonia, w/o Tulana, Goose Bay or ALB
                                        date_time < ymd("2007-10-30") ~ cap_taf_s0, # UKL w/o Caledonia, Tulana, Goose Bay or ALB
                                        date_time < ymd("2008-11-18") ~ cap_taf_s2, # UKL w/ Tulana, w/o Caledonia, Goose Bay or ALB
                                        date_time < ymd("2025-01-02") ~ cap_taf_s3, # UKL w/ Tulana and Goose Bay, w/o Caledonia or ALB
                                        date_time >= ymd("2025-01-02") ~ cap_taf_s4) # UKL w/ Tulana, Goose Bay and ALB, w/o Caledonia
            ) |> 
  glimpse()
```

## Other Inflows and Outflows 

### Link River - Keno Canal

Import Keno Canal supplemental flows

Source: “Final timeseries for modeling” tab, “Keno canal analysis Nov2016 update 7-1-2021.xlsx”

```{r}
keno_q <- read_csv(here::here("data-raw", "other_flows", "keno_suppl_flows.csv")) |>
  glimpse() 
```

Combine the different sources of Keno Canal flows:

```{r}
link_keno_flows <- 
  link_q |> 
  dplyr::select(date_time, link_cfs = value) |>
  left_join(link_keno_q |> 
              dplyr::select(date_time, link_keno_cfs = value),
            by = join_by(date_time)) |>
  left_join(keno_q |> 
              dplyr::select(date_time, keno_cfs),
            by = join_by(date_time)) |>
  mutate(water_year = year(date_time) + if_else(month(date_time) >= 10, 1, 0),
         day_of_wy = as.integer(as.Date(date_time) - ymd(paste(water_year - 1, 10, 1))) + 1) |>
  # if not available, assume 15 or 0 cfs based on time of year
  mutate(keno_cfs = coalesce(keno_cfs, if_else((day_of_wy >= 16) & (day_of_wy <= 182), 0, 15))) |>
  # if gage data for USGS 11507501 is available, use that instead
  mutate(keno_cfs = coalesce(link_keno_cfs - link_cfs, keno_cfs)) |>
  # propagate to the combined flow
  mutate(link_keno_cfs = link_cfs + keno_cfs) |>
  # convert to TF for the UKL net inflow calc
  mutate(link_keno_taf =link_keno_cfs * 1.983471 / 1000) |>
  glimpse()
```

### A Canal (USBR)

https://www.usbr.gov/uc/water/hydrodata/klamath_basin_data/site_map.html

**A Canal HDB**

https://www.usbr.gov/pn-bin/hdb/hdb.pl?svr=kbohdb&sdi=200356&tstp=DY&t1=2020-10-01T00:00&t2=2030-10-31T00:00&table=R&mrid=0&format=table

Site:		ACHO - A Canal Headworks
Parameter:		24-hr average flow
Interval:		Day
Source Provider:		USBR
Source Database:		USBR-KBHDB
Retrieval Database:		USBR-KBHDB
Units:		CFS

**A Canal Hydromet**

https://www.usbr.gov/pn-bin/daily.pl?station=acho&format=html&year=2020&month=10&day=1&year=2030&month=10&day=31&pcode=qj

Site:		ACHO - A Canal Headworks, cfs		

**A Canal Current WY**

https://www.usbr.gov/pn-bin/wyreport.pl?site=acho&parameter=qj&head=yes

**Preferred source: CSV**

```{r}
get_usbr_data <- function(site_number, parameter, workspace = here::here("data-raw", "usbr")) {
  
  filename_csv <- str_glue("usbr_{site_number}_{parameter}.csv")
  filename_Rds <- str_glue("usbr_{site_number}_{parameter}.Rds")
  
  csv_url <- str_glue("https://www.usbr.gov/uc/water/hydrodata/klamath_basin_data/{site_number}/csv/{parameter}.csv")
  
  if(!dir.exists(workspace)) {
    dir.create(workspace, recursive = T)
  }
  
  if(!file.exists(file.path(workspace, filename_Rds))) {
    httr::GET(url = csv_url, httr::write_disk(file.path(workspace, filename_csv), overwrite = T))
    data_raw <- read_csv(file.path(workspace, filename_csv)) 
    data_raw |> saveRDS(file.path(workspace, filename_Rds))
  } else {
    data_raw <- readRDS(file.path(workspace, filename_Rds))
  }
  
  data_raw
}

canal_a_flow <- 
  get_usbr_data(200062, 19) |>
  rename(date_time = datetime, canal_a_cfs = flow) |>
  mutate(canal_a_taf = canal_a_cfs * 1.983471 / 1000) |>
  glimpse()
```

### Agency Lake Ranch

Manually coded flow time series. 

* Positive = to Agency Lake Ranch (gravity gate > total pumped)
* Negative = from Agency Lake Ranch (gravity gate < total pumped)

```{r}
alr_q <-
  read_csv(here::here("data-raw", "other_flows", "agency_lake_ranch_calcs.csv")) |>
  mutate(net_alr_cfs = coalesce(gravity_gate_to_alr_cfs, 0) - coalesce(total_pumped_cfs, 0),
         net_alr_taf = net_alr_cfs * 1.983471 / 1000) |>
  glimpse()
```

### Caledonia Marsh

Manually coded volume time series. (thousand acre-feet from Caledonia Marsh)

```{r}
caledonia_v <-
  read_csv(here::here("data-raw", "other_flows", "from_caledonia_marsh.csv")) |>
  glimpse()
```

## Combine All

```{r}
combined_net_inflow <- ukl_storage_ts |>
  left_join(link_keno_flows |>
              dplyr::select(date_time, link_keno_taf), 
            by = join_by(date_time)) |>
  left_join(canal_a_flow |>
              dplyr::select(date_time, canal_a_taf), 
            by = join_by(date_time)) |>
  left_join(alr_q |>
              dplyr::select(date_time, net_alr_taf), 
            by = join_by(date_time)) |>
  left_join(caledonia_v |>
              dplyr::select(date_time, from_caledonia_taf), 
            by = join_by(date_time)) |>
  # linear interpolate to fill gaps in lake capacity and link river flows
  complete(date_time = seq(min(date_time), max(date_time), by = "day")) |>
  mutate(across(c(ukl_act_cap_taf, link_keno_taf),
                \(y) zoo::na.approx(y, x = date_time, na.rm = F))) |>
  # fill gaps in the other net flow variables with zero
  mutate(across(c(canal_a_taf, net_alr_taf, from_caledonia_taf), 
                \(y) coalesce(y, 0))) |>
  # calculate net inflow
  mutate(ukl_storage_change_taf = (ukl_act_cap_taf - lag(ukl_act_cap_taf)),
         ukl_net_inflow = ukl_storage_change_taf + link_keno_taf + canal_a_taf + net_alr_taf - from_caledonia_taf) |>
  filter(row_number() > 1) |>
  glimpse()
```

```{r}
combined_net_inflow |>
  ggplot(aes(x = date_time)) +
  geom_line(aes(y = ukl_storage_change_taf, color = "Upper Klamath Lake")) + 
  geom_line(aes(y = link_keno_taf, color = "Link River and Keno Canal")) + 
  geom_line(aes(y = canal_a_taf, color = "A Canal")) + 
  geom_line(aes(y = net_alr_taf, color = "Agency Lake Ranch")) + 
  geom_line(aes(y = from_caledonia_taf, color = "Caledonia Marsh")) +
  ylab("change in storage (thousand acre-feet)")
```

```{r}
combined_net_inflow |>
  ggplot(aes(x = date_time)) +
  geom_line(aes(y = cumsum(ukl_storage_change_taf))) +
  ylab("cumulative storage relative to start (thousand acre-feet)")
```

### Smoothing

alpha = 0.182 is currently hard-coded. Actual method uses optimization (gradient descent?) to find the alpha that minimizes MAE

```{r}
exp_smooth_ts <- function(ts, alpha) {
  Reduce(function(prev, curr) alpha * curr + (1 - alpha) * prev, 
         ts, accumulate = TRUE)
}

combined_net_inflow_smooth <-
  combined_net_inflow |>
  mutate(ukl_net_inflow_smoothed = exp_smooth_ts(ukl_net_inflow, alpha = 0.182)) |>
  glimpse()

combined_net_inflow_smooth |>
  head(100) |>
  ggplot(aes(x = date_time)) + 
  geom_line(aes(y = ukl_net_inflow, color = "raw")) + 
  geom_line(aes(y = ukl_net_inflow_smoothed, color = "smoothed"))
```
```{r}
# combined_net_inflow <- combined_net_inflow |> 
#   rename(date = date_time) |> 
#   mutate(date = as.Date(date, format= "%Y-%m-%d"),
#          date = format(date, "%m/%d/%Y")) 
combined_net_inflow_smooth <- combined_net_inflow_smooth |> 
  rename(date = date_time) |> 
  mutate(date = as.Date(date, format= "%Y-%m-%d"),
         date = format(date, "%m/%d/%Y"))
write_csv(combined_net_inflow_smooth, here::here("data/ukr_net_inflow/combined_net_inflow.csv"))
```
